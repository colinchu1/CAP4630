{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN73sGvAAe6gGHShf+f1wqa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/colinchu1/CAP4630/blob/master/HW_5/HW5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tim9Ca73jqp",
        "colab_type": "text"
      },
      "source": [
        "#General Concept"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8dCTDGqdVhV",
        "colab_type": "text"
      },
      "source": [
        "Artificial intelligence, machine learning, deep learning are subset of each other. deep learning is a subset of machine learning, and machine learning is a subset of artificial intelligence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePw9HwPFQguO",
        "colab_type": "text"
      },
      "source": [
        "##Artificial Intelligence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuWbev9uPkkH",
        "colab_type": "text"
      },
      "source": [
        "Artificial Intelligence is a branch of computer science dealing with the simulation of intelligent behavior in computers.\n",
        "The capability of a machine to imitate intelligent human behavior. A computer system able to perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision-making, and translation between languages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvimXyXyQo2N",
        "colab_type": "text"
      },
      "source": [
        "##Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b01R5HIQQs89",
        "colab_type": "text"
      },
      "source": [
        "Machine learning is field of study that gives computers the ability to learn without being explicitly programmed. Machine-learning programs, in a sense, adjust themselves in response to the data they're exposed to (like a child that is born knowing nothing adjusts its understanding of the world in response to experience). Machine learning is dynamic and does not require human intervention to make certain changes. That makes it less brittle, and less reliant on human experts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpnXAgQgRNyV",
        "colab_type": "text"
      },
      "source": [
        "##Deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aBDD_a-SFfd",
        "colab_type": "text"
      },
      "source": [
        "Deep learning is a subset of machine learning. Usually, when people use the term deep learning, they are referring to deep artificial neural networks, and somewhat less frequently to deep reinforcement learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T__WJCHZSHAl",
        "colab_type": "text"
      },
      "source": [
        "#Basic concepts "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7sKl00CYmXJ",
        "colab_type": "text"
      },
      "source": [
        "##Linear regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCLNZFTdfdyT",
        "colab_type": "text"
      },
      "source": [
        "Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable, and the other is considered to be a dependent variable.\n",
        "\n",
        "A linear regression line has an equation of the form Y = a + bX, where X is the explanatory variable and Y is the dependent variable. The slope of the line is b, and a is the intercept (the value of y when x = 0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrPJx0JihVLr",
        "colab_type": "code",
        "outputId": "a34e9752-fcec-4c11-a89c-39d500d12e76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "  \n",
        "def estimate_coef(x, y): \n",
        "    # number of observations/points \n",
        "    n = np.size(x) \n",
        "  \n",
        "    # mean of x and y vector \n",
        "    m_x, m_y = np.mean(x), np.mean(y) \n",
        "  \n",
        "    # calculating cross-deviation and deviation about x \n",
        "    SS_xy = np.sum(y*x) - n*m_y*m_x \n",
        "    SS_xx = np.sum(x*x) - n*m_x*m_x \n",
        "  \n",
        "    # calculating regression coefficients \n",
        "    b_1 = SS_xy / SS_xx \n",
        "    b_0 = m_y - b_1*m_x \n",
        "  \n",
        "    return(b_0, b_1) \n",
        "  \n",
        "def plot_regression_line(x, y, b): \n",
        "    # plotting the actual points as scatter plot \n",
        "    plt.scatter(x, y, color = \"m\", \n",
        "               marker = \"o\", s = 30) \n",
        "  \n",
        "    # predicted response vector \n",
        "    y_pred = b[0] + b[1]*x \n",
        "  \n",
        "    # plotting the regression line \n",
        "    plt.plot(x, y_pred, color = \"g\") \n",
        "  \n",
        "    # putting labels \n",
        "    plt.xlabel('x') \n",
        "    plt.ylabel('y') \n",
        "  \n",
        "    # function to show plot \n",
        "    plt.show() \n",
        "  \n",
        "def main(): \n",
        "    # observations \n",
        "    x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) \n",
        "    y = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12]) \n",
        "  \n",
        "    # estimating coefficients \n",
        "    b = estimate_coef(x, y) \n",
        "    print(\"Estimated coefficients:\\nb_0 = {}  \\\\nb_1 = {}\".format(b[0], b[1])) \n",
        "  \n",
        "    # plotting regression line \n",
        "    plot_regression_line(x, y, b) \n",
        "  \n",
        "if __name__ == \"__main__\": \n",
        "    main() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Estimated coefficients:\n",
            "b_0 = 1.2363636363636363  \\nb_1 = 1.1696969696969697\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0isdn8UJhAHb",
        "colab_type": "text"
      },
      "source": [
        "source: geeksforgeeks.org/linear-regression-python-implementation/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h91hm29qh1fE",
        "colab_type": "text"
      },
      "source": [
        "##Logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNkoqN2GiPDj",
        "colab_type": "text"
      },
      "source": [
        "Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression is estimating the parameters of a logistic model (a form of binary regression). Mathematically, a binary logistic model has a dependent variable with two possible values, such as pass/fail which is represented by an indicator variable, where the two values are labeled \"0\" and \"1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxPg_3xriRe7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7F1cAJ6nwXz",
        "colab_type": "text"
      },
      "source": [
        "###Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K3PBwFhiazv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pb7S1G1nuCB",
        "colab_type": "text"
      },
      "source": [
        "###Get random data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GatR6jFFnfum",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_random_data(w, b, mu, sigma, m):\n",
        "  data = [[] for j in range(m)] \n",
        "  labels = []\n",
        "  for x in range (0,m):\n",
        "    c = random.randint(0,1)\n",
        "    x_1 = np.random.uniform()\n",
        "    n = np.random.normal(mu, sigma)\n",
        "    x_2 = w * x_1 + b + (-1)**c * n\n",
        "    data[x].append(x_1)\n",
        "    data[x].append(x_2)\n",
        "    labels.append(c)\n",
        "  return data, labels\n",
        "\n",
        "mu , sigma = 1.5,0.5\n",
        "m = 500\n",
        "w = 2\n",
        "b = 0\n",
        "data, labels = get_random_data(w,b,mu,sigma,m)\n",
        "data = np.array(data)\n",
        "labels = np.array(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b8XLiixn078",
        "colab_type": "text"
      },
      "source": [
        "###Split data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koSrZBp_nzSk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, labels_train, labels_test = train_test_split(data, labels, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-QwrtEkn3e1",
        "colab_type": "text"
      },
      "source": [
        "###Predict function, logistic regression is to predict if the data is 1 or 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXhmfWhcn3yc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(a):\n",
        "    return 0 if a < 0.5 else 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ol5wZgzaoF1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid (z):\n",
        "    return 1 / (1 + np.exp(-z))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-GcCcWYoeH5",
        "colab_type": "text"
      },
      "source": [
        "### loss function how far off is the prediction compare to the true value\n",
        "this is a binary crossentropy loss function used for logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T_UCREgofbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binary_crossentropy(a, label):\n",
        "    return -label*np.log(a) - (1 - label)*np.log(1 - a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP2udYjto5xM",
        "colab_type": "text"
      },
      "source": [
        "###Logistic regression and test model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rrsVFYpoysU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_model(test_data, test_labels, W_b):\n",
        "    correct_predictions = 0\n",
        "    total_loss = 0\n",
        "    testd = np.column_stack((test_data, np.ones(shape=(test_data.shape[0], 1))))\n",
        "    for i_data in range(len(test_data)):\n",
        "        # Process the input\n",
        "        z = np.dot(testd[i_data], W_b)\n",
        "        a = sigmoid(z)\n",
        "\n",
        "        # Make a prediction\n",
        "        p = predict(a)\n",
        "        if p == test_labels[i_data]:\n",
        "            correct_predictions += 1\n",
        "        \n",
        "        # Determine loss\n",
        "        total_loss += binary_crossentropy(a, test_labels[i_data])\n",
        "    # Return a summary\n",
        "    print(total_loss)\n",
        "    return (total_loss / len(test_data), correct_predictions / len(test_data))\n",
        "def logistic_regression(train_data, train_labels, test_data, test_labels, epochs, learing_rate):\n",
        "    # Randomize the initial weights\n",
        "    W_b = np.random.random_sample((3, ))\n",
        "    #combine data with biases\n",
        "    trd = np.column_stack((X_train, np.ones(shape=(X_train.shape[0], 1))))\n",
        "    for epoch in range(epochs):\n",
        "        # Only perform stochastic gradient descent\n",
        "        for i_data in range(len(train_data)):\n",
        "            # Process the input\n",
        "            z = np.dot(trd[i_data], W_b)\n",
        "            a = sigmoid(z)\n",
        "\n",
        "            # Determine the gradient of the loss\n",
        "            Lg_b = (a- train_labels[i_data])* trd[i_data]\n",
        "\n",
        "            # Apply the gradient to the weights\n",
        "            W_b -= Lg_b * learning_rate\n",
        "        \n",
        "        # Analyze the loss and accuracy for each epoch\n",
        "        loss, accuracy = test_model(test_data, test_labels, W_b)\n",
        "        print(f'Epoch {epoch+1}/{epochs} - val_loss: {loss} - val_accuracy: {accuracy}')\n",
        "    \n",
        "    # Return the trained weights\n",
        "    return W_b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5xEA1gLo-hz",
        "colab_type": "text"
      },
      "source": [
        "Doing logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0vgWlz7pA_k",
        "colab_type": "code",
        "outputId": "9bd38282-8a14-488d-90de-983b6270014b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "epochs = 10\n",
        "learning_rate = 0.01\n",
        "\n",
        "W_b = logistic_regression(X_train, labels_train, X_test, labels_test, epochs, learning_rate)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20.501220498190257\n",
            "Epoch 1/10 - val_loss: 0.20501220498190256 - val_accuracy: 0.97\n",
            "15.68248720256308\n",
            "Epoch 2/10 - val_loss: 0.15682487202563078 - val_accuracy: 0.97\n",
            "13.175211392612159\n",
            "Epoch 3/10 - val_loss: 0.1317521139261216 - val_accuracy: 0.97\n",
            "11.582010724848129\n",
            "Epoch 4/10 - val_loss: 0.11582010724848128 - val_accuracy: 0.97\n",
            "10.472296603493962\n",
            "Epoch 5/10 - val_loss: 0.10472296603493962 - val_accuracy: 0.97\n",
            "9.65219751802457\n",
            "Epoch 6/10 - val_loss: 0.0965219751802457 - val_accuracy: 0.97\n",
            "9.019659132628556\n",
            "Epoch 7/10 - val_loss: 0.09019659132628556 - val_accuracy: 0.97\n",
            "8.515624074861742\n",
            "Epoch 8/10 - val_loss: 0.08515624074861743 - val_accuracy: 0.97\n",
            "8.10352828327272\n",
            "Epoch 9/10 - val_loss: 0.08103528283272719 - val_accuracy: 0.97\n",
            "7.759525457112547\n",
            "Epoch 10/10 - val_loss: 0.07759525457112547 - val_accuracy: 0.97\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpRFPpF0pcIZ",
        "colab_type": "text"
      },
      "source": [
        "###Gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBw-MVn8qTQ7",
        "colab_type": "text"
      },
      "source": [
        "the Gradient is a vector-valued function that stores partial derivatives. In other words, the gradient is a vector, and each of its components is a partial derivative with respect to one specific variable.\n",
        "Take the function, f(x, y) = 2x² + y² as another example.\n",
        "Here, f(x, y) is a multi-variable function. It would be f(x,y) = [4x,2y]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9AIq99-qlvr",
        "colab_type": "text"
      },
      "source": [
        "###Gradient descent\n",
        "Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WStV3QfQrqTj",
        "colab_type": "text"
      },
      "source": [
        "In above logistic regression model our gradient descent is"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQyUGMGxsrwE",
        "colab_type": "text"
      },
      "source": [
        " find the loss: Lg_b = (a- train_labels[i_data])* trd[i_data]\n",
        "\n",
        " apply the loss gradient: W_b -= Lg_b * learning_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4gG-4vjJ18o",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> for gradient descent there is batch, Mini-batch, and Stochastic\n",
        "\n",
        "> batch - use all the data\n",
        "\n",
        "> Mini-batch - use a subset of the data\n",
        "\n",
        "> stochastic = use 1 element from the data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkawuR_dsvJ8",
        "colab_type": "text"
      },
      "source": [
        "##Building a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnLy_dKjvPbz",
        "colab_type": "text"
      },
      "source": [
        "In this class we have mainly used keras model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2NoKX-g0o77",
        "colab_type": "text"
      },
      "source": [
        "To build a keras only require a few line of code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-1W6U1m0s5_",
        "colab_type": "code",
        "outputId": "fb267f62-eaba-4eb9-eead-36fca233da38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1a0Fqxi0wOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = models.Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MylNPBvX4sg6",
        "colab_type": "text"
      },
      "source": [
        "###Convolution 2D layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOmLa7ay4jWq",
        "colab_type": "text"
      },
      "source": [
        "This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None, it is applied to the outputs as well.\n",
        "\n",
        "Given an input matrix of $\\begin{bmatrix}\n",
        "1, 0, 0, 0 \\\\\n",
        "0, 1, 0, 0 \\\\\n",
        "0, 0, 1, 0 \\\\\n",
        "0, 0, 0, 1\n",
        "\\end{bmatrix}$ , and a kernel of $\\begin{bmatrix}\n",
        "1, 0 \\\\\n",
        "0, 1\n",
        "\\end{bmatrix}$ would produce an output of $\\begin{bmatrix}\n",
        "2, 0, 0 \\\\\n",
        "0, 2, 0 \\\\\n",
        "0, 0, 2\n",
        "\\end{bmatrix}$\n",
        "\n",
        "below code is how a simple conv2d layer will work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWQ606vG5lL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv2d(input_mat, kernel_mat):\n",
        "  #if kernel matrix is bigger than input matrix operation can not be perform\n",
        "  #return empty matrix\n",
        "  out = []\n",
        "  if(input_mat.shape[0]<kernel_mat.shape[0]):\n",
        "    print(\"error, operation can not be performed\")\n",
        "    return out\n",
        "  #sub shape will be the size of kernel matrix\n",
        "  sub_shape = kernel_mat.shape\n",
        "  view_shape = tuple(np.subtract(input_mat.shape, sub_shape) + 1) + sub_shape\n",
        "  strides = input_mat.strides + input_mat.strides\n",
        "  #create all the sub matrix\n",
        "  sub_matrices = np.lib.stride_tricks.as_strided(input_mat,view_shape,strides)\n",
        "  #using einsum top find the result matrix\n",
        "  out = np.einsum('xyij,ij->xy',sub_matrices,kernel_mat)\n",
        "  \n",
        "  return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAMvRs5_AboZ",
        "colab_type": "text"
      },
      "source": [
        "###Flatten layer\n",
        "Flatten is used to flatten the input. For example, if flatten is applied to layer having input shape as (batch_size, 2,2), then the output shape of the layer will be (batch_size, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrumHMMa4qhi",
        "colab_type": "text"
      },
      "source": [
        "###Dense layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arnHGjMD4g9s",
        "colab_type": "text"
      },
      "source": [
        "Dense layer is the regular deeply connected neural network layer. It is most common and frequently used layer. Dense layer does the below operation on the input and return the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDGPyonlBK6h",
        "colab_type": "text"
      },
      "source": [
        "###Dropout layer\n",
        "Dropout is a regularization technique patented by Google for reducing overfitting in neural networks by preventing complex co-adaptations on training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXOHV_t4BhmZ",
        "colab_type": "text"
      },
      "source": [
        "##Activation function\n",
        "Neural network activation functions are a crucial component of deep learning. Activation functions determine the output of a deep learning model, its accuracy, and also the computational efficiency of training a model—which can make or break a large scale neural network. \n",
        "\n",
        "below we will use **relu** and **sigmoid**\n",
        "\n",
        "**Relu** - The Rectified Linear Unit is the most commonly used activation function in deep learning models. The function returns 0 if it receives any negative input, but for any positive value  x  it returns that value back. So it can be written as  f(x)=max(0,x) .\n",
        "\n",
        "**Sigmoid** - The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output.Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfeHF_HPCgXP",
        "colab_type": "text"
      },
      "source": [
        "###This is a convolutional base i chose mobilenetV2\n",
        "inside a convolutional base it contains many layers like conv2d, maxpooling and more"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIf286xLCQAD",
        "colab_type": "code",
        "outputId": "7b6481d9-1040-4014-e1d0-9eceffe329c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "from keras.applications import MobileNetV2\n",
        "\n",
        "conv_base = MobileNetV2(\n",
        "    weights='imagenet', \n",
        "    include_top=False, \n",
        "    input_shape=(150, 150, 3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_applications/mobilenet_v2.py:294: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  warnings.warn('`input_shape` is undefined or non-square, '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/JonathanCMitchell/mobilenet_v2_keras/releases/download/v1.1/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9412608/9406464 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nquCV4YdMrm",
        "colab_type": "text"
      },
      "source": [
        "Freezeing the base for future fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z742YJNDdKbm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_base.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24z6tdiPD_4r",
        "colab_type": "text"
      },
      "source": [
        "We will add the base to all the previously mentioned layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yim5aVNrBXxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(conv_base)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dropout(0.1))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ycs9tHhvEHO9",
        "colab_type": "code",
        "outputId": "69808a21-88d2-4f64-f9cd-32c95625abfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "mobilenetv2_1.00_224 (Model) (None, 5, 5, 1280)        2257984   \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 32000)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               8192256   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 10,450,497\n",
            "Trainable params: 8,192,513\n",
            "Non-trainable params: 2,257,984\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi_i91ybESlB",
        "colab_type": "text"
      },
      "source": [
        "##Compiling the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqDmUJXCFK7o",
        "colab_type": "text"
      },
      "source": [
        "###When compiling a model you need to choose a loss function and a optimizers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0FhTOaLGcMq",
        "colab_type": "text"
      },
      "source": [
        "The Loss Function is one of the important components of Neural Networks. Loss is nothing but a prediction error of Neural Net. And the method to calculate the loss is called Loss Function. In simple words, the Loss is used to calculate the gradients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3QbnHYlx6LU",
        "colab_type": "text"
      },
      "source": [
        "**Cross-entropy loss**, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVkinzWWKvoi",
        "colab_type": "text"
      },
      "source": [
        "optimizers shape and mold your model into its most accurate possible form by futzing with the weights. The loss function is the guide to the terrain, telling the optimizer when it’s moving in the right or wrong direction.\n",
        "\n",
        "Noteable Optimizer\n",
        "\n",
        "*   Adam\n",
        "*   RMSprop (root mean square)\n",
        "*   SGD(Stochastic gradient descent)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5voA0o3Pwxx",
        "colab_type": "text"
      },
      "source": [
        "in side  optimizer there is a paramerter called learning rate\n",
        "\n",
        "the learning rate is a configurable hyperparameter used in the training of neural networks that has a small positive value, often in the range between 0.0 and 1.0. The learning rate controls how quickly the model is adapted to the problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyX-M4rWEJ_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "    loss='binary_crossentropy', \n",
        "    optimizer=optimizers.RMSprop(lr=2e-5), \n",
        "    metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnypQwvCEWcY",
        "colab_type": "text"
      },
      "source": [
        "#training a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK-irGYGU_Zn",
        "colab_type": "text"
      },
      "source": [
        "We will use the same data from Home work 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJw4pNpLTiZJ",
        "colab_type": "code",
        "outputId": "a1ae4ca0-c8dc-453b-a219-8a195005f632",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "    -O /tmp/cats_and_dogs_filtered.zip\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-22 04:06:45--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.204.128, 2404:6800:4008:c01::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.204.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘/tmp/cats_and_dogs_filtered.zip’\n",
            "\n",
            "/tmp/cats_and_dogs_ 100%[===================>]  65.43M  76.8MB/s    in 0.9s    \n",
            "\n",
            "2020-04-22 04:06:46 (76.8 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbaKAdJATi7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Directory with our training cat pictures\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "\n",
        "# Directory with our training dog pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "\n",
        "# Directory with our validation cat pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "\n",
        "# Directory with our validation dog pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlUwicyqT01u",
        "colab_type": "code",
        "outputId": "662d4648-49ed-4c86-8f4a-9387f929e174",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255, \n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode='binary')\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode='binary')\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=50,\n",
        "    epochs=30,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=50\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n",
            "Epoch 1/30\n",
            "50/50 [==============================] - 22s 436ms/step - loss: 0.4868 - acc: 0.7770 - val_loss: 0.1998 - val_acc: 0.9070\n",
            "Epoch 2/30\n",
            "50/50 [==============================] - 12s 240ms/step - loss: 0.3870 - acc: 0.8210 - val_loss: 0.2986 - val_acc: 0.8520\n",
            "Epoch 3/30\n",
            "50/50 [==============================] - 12s 240ms/step - loss: 0.3340 - acc: 0.8610 - val_loss: 0.4033 - val_acc: 0.9240\n",
            "Epoch 4/30\n",
            "50/50 [==============================] - 12s 240ms/step - loss: 0.3037 - acc: 0.8670 - val_loss: 0.6456 - val_acc: 0.9030\n",
            "Epoch 5/30\n",
            "50/50 [==============================] - 12s 242ms/step - loss: 0.2745 - acc: 0.8900 - val_loss: 0.0055 - val_acc: 0.9220\n",
            "Epoch 6/30\n",
            "50/50 [==============================] - 12s 241ms/step - loss: 0.2928 - acc: 0.8780 - val_loss: 0.7334 - val_acc: 0.8770\n",
            "Epoch 7/30\n",
            "50/50 [==============================] - 12s 240ms/step - loss: 0.2914 - acc: 0.8740 - val_loss: 0.0019 - val_acc: 0.9310\n",
            "Epoch 8/30\n",
            "50/50 [==============================] - 12s 240ms/step - loss: 0.3108 - acc: 0.8740 - val_loss: 0.6273 - val_acc: 0.8780\n",
            "Epoch 9/30\n",
            "50/50 [==============================] - 12s 243ms/step - loss: 0.2706 - acc: 0.8900 - val_loss: 1.0769 - val_acc: 0.9090\n",
            "Epoch 10/30\n",
            "50/50 [==============================] - 12s 239ms/step - loss: 0.2654 - acc: 0.8860 - val_loss: 0.0309 - val_acc: 0.9320\n",
            "Epoch 11/30\n",
            "50/50 [==============================] - 12s 240ms/step - loss: 0.2698 - acc: 0.8960 - val_loss: 0.3336 - val_acc: 0.8900\n",
            "Epoch 12/30\n",
            "50/50 [==============================] - 12s 242ms/step - loss: 0.2479 - acc: 0.8900 - val_loss: 1.4963 - val_acc: 0.8900\n",
            "Epoch 13/30\n",
            "50/50 [==============================] - 12s 244ms/step - loss: 0.2801 - acc: 0.8810 - val_loss: 0.1762 - val_acc: 0.9270\n",
            "Epoch 14/30\n",
            "50/50 [==============================] - 12s 238ms/step - loss: 0.2372 - acc: 0.8990 - val_loss: 0.2425 - val_acc: 0.9230\n",
            "Epoch 15/30\n",
            "50/50 [==============================] - 12s 239ms/step - loss: 0.2704 - acc: 0.8800 - val_loss: 0.1670 - val_acc: 0.9390\n",
            "Epoch 16/30\n",
            "50/50 [==============================] - 12s 237ms/step - loss: 0.2392 - acc: 0.8960 - val_loss: 0.0146 - val_acc: 0.9220\n",
            "Epoch 17/30\n",
            "50/50 [==============================] - 12s 240ms/step - loss: 0.2619 - acc: 0.8960 - val_loss: 0.3832 - val_acc: 0.9080\n",
            "Epoch 18/30\n",
            "50/50 [==============================] - 12s 236ms/step - loss: 0.2291 - acc: 0.9060 - val_loss: 0.3933 - val_acc: 0.8800\n",
            "Epoch 19/30\n",
            "50/50 [==============================] - 12s 234ms/step - loss: 0.2849 - acc: 0.8900 - val_loss: 0.0120 - val_acc: 0.9480\n",
            "Epoch 20/30\n",
            "50/50 [==============================] - 12s 234ms/step - loss: 0.2308 - acc: 0.9050 - val_loss: 0.9974 - val_acc: 0.8950\n",
            "Epoch 21/30\n",
            "50/50 [==============================] - 12s 235ms/step - loss: 0.2183 - acc: 0.9100 - val_loss: 0.9942 - val_acc: 0.8810\n",
            "Epoch 22/30\n",
            "50/50 [==============================] - 12s 238ms/step - loss: 0.2298 - acc: 0.9040 - val_loss: 0.2781 - val_acc: 0.9150\n",
            "Epoch 23/30\n",
            "50/50 [==============================] - 12s 239ms/step - loss: 0.2371 - acc: 0.9000 - val_loss: 0.3898 - val_acc: 0.9190\n",
            "Epoch 24/30\n",
            "50/50 [==============================] - 12s 241ms/step - loss: 0.2526 - acc: 0.8960 - val_loss: 1.9907 - val_acc: 0.8790\n",
            "Epoch 25/30\n",
            "50/50 [==============================] - 12s 240ms/step - loss: 0.2189 - acc: 0.9110 - val_loss: 0.0116 - val_acc: 0.9240\n",
            "Epoch 26/30\n",
            "50/50 [==============================] - 12s 240ms/step - loss: 0.2444 - acc: 0.8810 - val_loss: 0.3850 - val_acc: 0.8850\n",
            "Epoch 27/30\n",
            "50/50 [==============================] - 12s 240ms/step - loss: 0.2555 - acc: 0.8910 - val_loss: 0.1787 - val_acc: 0.8830\n",
            "Epoch 28/30\n",
            "50/50 [==============================] - 12s 240ms/step - loss: 0.2433 - acc: 0.9020 - val_loss: 0.2344 - val_acc: 0.9070\n",
            "Epoch 29/30\n",
            "50/50 [==============================] - 12s 242ms/step - loss: 0.2450 - acc: 0.8990 - val_loss: 1.5632 - val_acc: 0.8970\n",
            "Epoch 30/30\n",
            "50/50 [==============================] - 12s 239ms/step - loss: 0.2251 - acc: 0.9060 - val_loss: 0.0634 - val_acc: 0.9120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZJRKmwQWsNn",
        "colab_type": "text"
      },
      "source": [
        "Printing validation loss and accruacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_LAwiDFXDeL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_loss, val_acc = model.evaluate_generator(validation_generator, steps=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYthaATXYOLZ",
        "colab_type": "code",
        "outputId": "4e1124ca-00e8-403e-cbfb-5f73de465e02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(\"Validation loss:\", val_loss)\n",
        "print(\"Validation accuracy:\", val_acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.276877224445343\n",
            "Validation accuracy: 0.9120000004768372\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIBEb3g3jx7e",
        "colab_type": "text"
      },
      "source": [
        "#Fine-Tuning\n",
        "\n",
        "Fine tuning machine learning predictive model is a crucial step to improve accuracy of the forecasted results. \n",
        "\n",
        "In upcoming example we will unfreeze a layer and train it again"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj287rx6DUjO",
        "colab_type": "text"
      },
      "source": [
        "Unfreezing block_5_expand layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGg47hkf2_YU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_base.trainable = True\n",
        "\n",
        "set_trainable = False\n",
        "for layer in conv_base.layers:\n",
        "  if layer.name == 'block_5_expand':\n",
        "    set_trainable = True\n",
        "  if set_trainable:\n",
        "    layer.trainable = True\n",
        "  else:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXGFW7bE3D8a",
        "colab_type": "code",
        "outputId": "bf8a3d27-258b-47b5-d974-ca92fa474862",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# compile model\n",
        "\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    #\n",
        "    # choose a smaller learning rate\n",
        "    #\n",
        "    optimizer=optimizers.RMSprop(lr=1e-5), \n",
        "    metrics=['acc'])\n",
        "\n",
        "# train\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=50,\n",
        "    epochs=30,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "50/50 [==============================] - 25s 500ms/step - loss: 0.2019 - acc: 0.9110 - val_loss: 0.3797 - val_acc: 0.9110\n",
            "Epoch 2/30\n",
            "50/50 [==============================] - 14s 283ms/step - loss: 0.2020 - acc: 0.9190 - val_loss: 0.1539 - val_acc: 0.9190\n",
            "Epoch 3/30\n",
            "50/50 [==============================] - 14s 282ms/step - loss: 0.1776 - acc: 0.9290 - val_loss: 0.2176 - val_acc: 0.9260\n",
            "Epoch 4/30\n",
            "50/50 [==============================] - 14s 285ms/step - loss: 0.1851 - acc: 0.9220 - val_loss: 0.4765 - val_acc: 0.9250\n",
            "Epoch 5/30\n",
            "50/50 [==============================] - 14s 284ms/step - loss: 0.1757 - acc: 0.9260 - val_loss: 0.1121 - val_acc: 0.9360\n",
            "Epoch 6/30\n",
            "50/50 [==============================] - 14s 283ms/step - loss: 0.1432 - acc: 0.9430 - val_loss: 0.0311 - val_acc: 0.9350\n",
            "Epoch 7/30\n",
            "50/50 [==============================] - 14s 285ms/step - loss: 0.1895 - acc: 0.9250 - val_loss: 0.0282 - val_acc: 0.9420\n",
            "Epoch 8/30\n",
            "50/50 [==============================] - 14s 283ms/step - loss: 0.1869 - acc: 0.9210 - val_loss: 0.1295 - val_acc: 0.9340\n",
            "Epoch 9/30\n",
            "50/50 [==============================] - 14s 283ms/step - loss: 0.1702 - acc: 0.9310 - val_loss: 0.1908 - val_acc: 0.9410\n",
            "Epoch 10/30\n",
            "50/50 [==============================] - 14s 285ms/step - loss: 0.1716 - acc: 0.9230 - val_loss: 0.0190 - val_acc: 0.9500\n",
            "Epoch 11/30\n",
            "50/50 [==============================] - 14s 283ms/step - loss: 0.1516 - acc: 0.9360 - val_loss: 0.0935 - val_acc: 0.9480\n",
            "Epoch 12/30\n",
            "50/50 [==============================] - 14s 284ms/step - loss: 0.2008 - acc: 0.9270 - val_loss: 0.0718 - val_acc: 0.9570\n",
            "Epoch 13/30\n",
            "50/50 [==============================] - 14s 280ms/step - loss: 0.1773 - acc: 0.9300 - val_loss: 0.0164 - val_acc: 0.9550\n",
            "Epoch 14/30\n",
            "50/50 [==============================] - 14s 284ms/step - loss: 0.1641 - acc: 0.9340 - val_loss: 0.6505 - val_acc: 0.9520\n",
            "Epoch 15/30\n",
            "50/50 [==============================] - 14s 285ms/step - loss: 0.1556 - acc: 0.9390 - val_loss: 0.0944 - val_acc: 0.9500\n",
            "Epoch 16/30\n",
            "50/50 [==============================] - 14s 281ms/step - loss: 0.1486 - acc: 0.9460 - val_loss: 0.0834 - val_acc: 0.9590\n",
            "Epoch 17/30\n",
            "50/50 [==============================] - 14s 285ms/step - loss: 0.2096 - acc: 0.9230 - val_loss: 0.3602 - val_acc: 0.9610\n",
            "Epoch 18/30\n",
            "50/50 [==============================] - 14s 283ms/step - loss: 0.1053 - acc: 0.9580 - val_loss: 0.1547 - val_acc: 0.9610\n",
            "Epoch 19/30\n",
            "50/50 [==============================] - 14s 283ms/step - loss: 0.1435 - acc: 0.9500 - val_loss: 0.2375 - val_acc: 0.9610\n",
            "Epoch 20/30\n",
            "50/50 [==============================] - 14s 284ms/step - loss: 0.1343 - acc: 0.9460 - val_loss: 0.0738 - val_acc: 0.9650\n",
            "Epoch 21/30\n",
            "50/50 [==============================] - 14s 284ms/step - loss: 0.1391 - acc: 0.9370 - val_loss: 0.0245 - val_acc: 0.9620\n",
            "Epoch 22/30\n",
            "50/50 [==============================] - 14s 286ms/step - loss: 0.1586 - acc: 0.9320 - val_loss: 0.0678 - val_acc: 0.9590\n",
            "Epoch 23/30\n",
            "50/50 [==============================] - 14s 283ms/step - loss: 0.1568 - acc: 0.9450 - val_loss: 0.0054 - val_acc: 0.9630\n",
            "Epoch 24/30\n",
            "50/50 [==============================] - 14s 282ms/step - loss: 0.1670 - acc: 0.9350 - val_loss: 0.0094 - val_acc: 0.9610\n",
            "Epoch 25/30\n",
            "50/50 [==============================] - 14s 283ms/step - loss: 0.1275 - acc: 0.9490 - val_loss: 0.0270 - val_acc: 0.9650\n",
            "Epoch 26/30\n",
            "50/50 [==============================] - 14s 283ms/step - loss: 0.1472 - acc: 0.9450 - val_loss: 0.3933 - val_acc: 0.9640\n",
            "Epoch 27/30\n",
            "50/50 [==============================] - 14s 285ms/step - loss: 0.1537 - acc: 0.9300 - val_loss: 0.1133 - val_acc: 0.9660\n",
            "Epoch 28/30\n",
            "50/50 [==============================] - 14s 281ms/step - loss: 0.0863 - acc: 0.9660 - val_loss: 0.1710 - val_acc: 0.9630\n",
            "Epoch 29/30\n",
            "50/50 [==============================] - 14s 285ms/step - loss: 0.1094 - acc: 0.9590 - val_loss: 0.0039 - val_acc: 0.9650\n",
            "Epoch 30/30\n",
            "50/50 [==============================] - 14s 283ms/step - loss: 0.1162 - acc: 0.9530 - val_loss: 0.2288 - val_acc: 0.9650\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekTxOeS9DYxR",
        "colab_type": "text"
      },
      "source": [
        "printing the result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4NGi8baDP_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_loss, val_acc = model.evaluate_generator(validation_generator, steps=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W15eCCXDDSIg",
        "colab_type": "code",
        "outputId": "477200a9-b708-446b-fd17-f8c4653002b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(\"Validation loss:\", val_loss)\n",
        "print(\"Validation accuracy:\", val_acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.0005004612030461431\n",
            "Validation accuracy: 0.9649999737739563\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYuPSsw9I5AR",
        "colab_type": "text"
      },
      "source": [
        "**there is a big increase in accuracy from 91% tp 96.5%**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "astoL_NNJFwI",
        "colab_type": "text"
      },
      "source": [
        "In summary for this course i learned a lot of tools that i can use in the future of my career. all type of gradient descent, keras model, linear regression, logistic regression. Max pooling, convolution network, conv2d. all these are really useful knowledge. through out the course i also picked up python to learn all the algorithm"
      ]
    }
  ]
}